# ШПОРА: Пропуски (Missing Values) и Категории

## 1. Работа с пропусками (Missing Values)
*Проблема: Модели (матрицы) не переваривают `NaN`.*

### А. Простые стратегии (Baseline)
| Метод | Суть | Когда применять | Минусы |
| :--- | :--- | :--- | :--- |
| **Логическая замена** | Категории $\to$ `'Unknown'`<br>Числа $\to$ `-1`, `0`, `999` | Когда пропуск имеет смысл (н-р, нет данных = нет кредита). | Может создать ложные зависимости. |
| **Статистическая** | `Mean` (среднее), `Median` (медиана), `Mode` (мода) | Быстро, просто. Медиана устойчива к выбросам. | **Искажает распределение** (делает пик в центре). |
| **Удаление (Drop)** | Удалить строку или столбец. | Если пропусков **>50-70%** (столбец) или пропуск в **Target** (строка). | Теряем данные. |

### Б. Умные методы (ML Imputation)
*Учитывают взаимосвязи между признаками.*
1.  **KNNImputer:** Ищет $K$ похожих строк (соседей) и берет их среднее.
    *   *Плюс:* Учитывает нелинейность.
    *   *Минус:* Медленный на больших данных.
2.  **MICE (IterativeImputer):** Для каждого столбца с пропусками строит регрессию, где этот столбец — целевая переменная, а остальные — признаки.
    *   *Важно:* **Не использовать целевую переменную (y)** для обучения импьютера (Data Leakage)!

---

## 2. Как оценить качество заполнения? (KL-Дивергенция)
Как понять, что мы не испортили данные заполнением? Сравниваем распределения.

**Метрика: KL-дивергенция (Кульбака–Лейблера)**
$$D_{KL}(p \parallel q) = \sum p(x) \ln \frac{p(x)}{q(x)}$$
*   $p(x)$ — распределение **до** заполнения (оригинал).
*   $q(x)$ — распределение **после** заполнения.

**Интерпретация:**
*   $D_{KL} \approx 0$ $\to$ Отлично (распределения совпали).
*   $D_{KL}$ растет $\to$ Плохо (мы сильно исказили данные).

**Пайплайн проверки:**
1.  Заполнить пропуски разными методами (Mean, Median, KNN).
2.  Построить гистограммы.
3.  Посчитать KL.
4.  Выбрать метод с наименьшим KL.

---

## 3. Кодирование категорий (Encoding)

### Типы категорий
1.  **Номинальные:** Нет порядка (Цвета: Красный, Синий).
2.  **Порядковые (Ordinal):** Есть порядок (Оценки: 2, 3, 4, 5; Размеры: S, M, L).

### Методы кодирования

| Метод | Как работает | Для кого | Плюсы/Минусы |
| :--- | :--- | :--- | :--- |
| **One-Hot Encoding** (OHE) | Столбец `Color` $\to$ `Color_Red`, `Color_Blue` (0/1) | Линейные модели, KNN, Нейросети | ✅ Не навязывает порядок.<br>❌ **Проклятие размерности** (если категорий >10-20). |
| **Label Encoding** | `A`$\to$1, `B`$\to$2, `C`$\to$3 | **Только Деревья** (RF, XGB, CatBoost) | ✅ Не плодит столбцы.<br>❌ Вводит ложный порядок (2 > 1), линейные модели сойдут с ума. |
| **Target Encoding** (Mean Encoding) | Заменяет категорию на среднее значение таргета по ней. | Бустинг, сложные задачи | ✅ Круто работает.<br>❌ **Жуткий риск переобучения**. Нужна регуляризация! |

---

## 4. Практические правила (Rules of Thumb)
1.  **Порог удаления:** Если в столбце **> 50-70%** пропусков — удаляй столбец смело. Информации там почти нет.
2.  **Target:** Если в строке пропущен **Target (y)** — строку удаляем. Учить не на чем.
3.  **Аномалии:** После заполнения могут появиться строки-аномалии (слишком много искусственных данных в одной строке). Их лучше дропнуть.
4.  **Разделение:**
    *   Анализ пропусков (% missing) можно делать на всем датасете.
    *   `fit` импьютеров и енкодеров — **строго на Train**.